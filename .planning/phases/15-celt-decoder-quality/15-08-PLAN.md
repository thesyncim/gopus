---
phase: 15-celt-decoder-quality
plan: 08
type: execute
wave: 2
depends_on: ["15-06"]
files_modified:
  - internal/celt/alloc_test.go
  - internal/testvectors/celt_debug_test.go
  - internal/celt/decoder_test.go
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Bit allocation produces reasonable values for typical frame sizes"
    - "Allocation totals match expected bit budget"
    - "Test vector packets can be decoded with trace showing allocation"
  artifacts:
    - path: "internal/celt/alloc_test.go"
      provides: "Bit allocation verification tests"
      contains: "TestComputeAllocationBudget"
    - path: "internal/testvectors/celt_debug_test.go"
      provides: "Test vector decoding with debug trace"
      contains: "TestCELTVectorWithTrace"
  key_links:
    - from: "internal/celt/alloc.go"
      to: "internal/celt/bands.go"
      via: "BandBits from allocation result"
      pattern: "allocResult.BandBits"
---

<objective>
Verify bit allocation correctness and trace CELT test vector decoding to identify divergence

Purpose: Bit allocation determines how many bits each band receives for PVQ encoding. If allocation is wrong, the range decoder will desynchronize (consume wrong number of bits), causing garbage output. This plan tests allocation and traces real test vector decoding.

Output: Allocation tests confirming budget correctness + trace of CELT test vector showing where divergence occurs
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-celt-decoder-quality/15-VERIFICATION.md

Key files:
@internal/celt/alloc.go (bit allocation algorithm)
@internal/celt/decoder.go (uses allocation to decode bands)
@internal/testvectors/compliance_test.go (EXISTING test vector infrastructure - 16KB)
@internal/testvectors/quality_test.go (EXISTING quality measurement - 8KB)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add bit allocation verification tests</name>
  <files>internal/celt/alloc_test.go</files>
  <action>
Add tests to alloc_test.go (create if needed) verifying bit allocation correctness.

1. TestComputeAllocationBudget:
   - For totalBits in [100, 500, 1000, 2000]:
     - Compute allocation for nbBands=21, lm=3 (20ms frame)
     - Verify sum(BandBits) + sum(FineBits) <= totalBits
     - Verify no negative allocations
     - Verify caps are respected (no band exceeds cap)

2. TestComputeAllocationDistribution:
   - Higher totalBits -> more bits to high-frequency bands
   - Lower bands (lower frequencies) get more bits at low bit rates
   - Allocation follows expected spectral shape

3. TestComputeAllocationTrim:
   - trim > 0 -> high bands get more bits
   - trim < 0 -> low bands get more bits
   - trim = 0 -> neutral

4. TestComputeAllocationByLM:
   - LM=0 (2.5ms) -> different caps than LM=3 (20ms)
   - Shorter frames have less total capacity

5. TestPulseCapsReasonable:
   - For each band, verify cap is proportional to band width
   - Wider bands can hold more pulses

Print allocation tables for manual inspection:
```go
t.Logf("Allocation for totalBits=%d, lm=%d:", totalBits, lm)
for band := 0; band < nbBands; band++ {
    t.Logf("  Band %2d: width=%2d, bandBits=%3d, fineBits=%2d, cap=%3d",
        band, ScaledBandWidth(band, 960), result.BandBits[band],
        result.FineBits[band], result.PulseCaps[band])
}
```
  </action>
  <verify>go test -v ./internal/celt/... -run TestComputeAllocation -count=1</verify>
  <done>Allocation tests pass, budget is respected, distribution is reasonable</done>
</task>

<task type="auto">
  <name>Task 2: Create CELT test vector trace test</name>
  <files>internal/testvectors/celt_debug_test.go</files>
  <action>
Create celt_debug_test.go with tests that decode CELT test vectors with tracing enabled. This is a NEW file that complements the EXISTING compliance_test.go (which runs compliance checks) and quality_test.go (which measures Q values). This file focuses specifically on diagnostic tracing.

1. TestCELTVectorWithTrace:
   - Load testvector01 (CELT stereo, mixed sizes) or testvector07 (CELT mono)
   - Enable LogTracer to capture output
   - Decode first 3 packets
   - Log trace output for each packet
   - Verify no decode errors

2. TestCELTSinglePacketDecode:
   - Extract a single packet from a CELT test vector
   - Decode with trace enabled
   - Log complete trace output
   - This provides a detailed view of one decode cycle

3. TestCELTEnergyProgression:
   - Decode multiple packets
   - Track energy values per band across packets
   - Log energy progression
   - Look for anomalies (sudden jumps, NaN, stuck values)

4. Compare trace output structure with expected libopus flow:
   - Should see: header -> coarse energy (21 bands) -> allocation -> fine energy -> PVQ per band -> synthesis
   - Flag if any stage is missing or has unexpected values

Use this test to identify:
- Which stage produces unexpected values
- Whether range decoder is consuming correct number of bits
- Whether energy values are in reasonable range (-30 to +10 dB typical)

The test output will be the key diagnostic for identifying Q=-100 root cause.
  </action>
  <verify>go test -v ./internal/testvectors/... -run TestCELTVector -count=1</verify>
  <done>CELT test vector trace test exists and produces diagnostic output</done>
</task>

<task type="auto">
  <name>Task 3: Add range decoder bit consumption tracking</name>
  <files>internal/celt/decoder_test.go</files>
  <action>
Add tests that track how many bits the range decoder consumes at each stage.

1. TestRangeDecoderBitConsumption:
   - Create a known CELT packet (from test vector)
   - Track rd.Tell() before and after each decode stage
   - Log: bits consumed for header, for coarse energy, for fine energy, for each band
   - Total consumed should equal packet size * 8 (approximately)

2. TestBitConsumptionVsAllocation:
   - Compute expected bits from allocation: sum(BandBits) + sum(FineBits) + header_overhead
   - Compare with actual bits consumed during decode
   - Significant mismatch indicates allocation/decode desync

3. Add to decoder.go (if not already):
   - After each major decode section, log Tell() position
   - This shows bit budget usage

Format for comparison:
```
Packet: 120 bytes = 960 bits
After header:     24 bits consumed
After coarse:    180 bits consumed (156 for energy)
After fine:      220 bits consumed (40 for fine)
After band 0:    248 bits consumed (28 for band 0)
After band 1:    280 bits consumed (32 for band 1)
...
End of decode:   958 bits consumed
```

If bits consumed per band don't match allocation, that's the bug.
  </action>
  <verify>go test -v ./internal/celt/... -run TestRangeDecoder -count=1</verify>
  <done>Bit consumption tracking tests exist and show consumption matches expected</done>
</task>

</tasks>

<verification>
- All allocation tests pass
- CELT test vector trace produces readable diagnostic output
- Bit consumption tracking shows bits consumed at each stage
- No panics or errors during test vector decoding
</verification>

<success_criteria>
- Bit allocation respects budget and produces reasonable distribution
- Test vector trace shows all decoding stages with intermediate values
- Bit consumption tracking reveals whether allocation matches actual decode
- Diagnostic information sufficient to identify Q=-100 root cause
</success_criteria>

<output>
After completion, create `.planning/phases/15-celt-decoder-quality/15-08-SUMMARY.md`
</output>
